\documentclass[modern]{aastex62}

% Load the corTeX style definitions
\input{cortex}

% Load custom style
\input{style}


% Bibliography
\bibliographystyle{aasjournal}

\usepackage{etoolbox}
\makeatletter % we need to patch \env@cases that has @ in its name
\patchcmd{\env@cases}{\quad}{\qquad\qquad}{}{}
\makeatother

\usepackage{enumitem}

% Begin!
\begin{document}

% Title
\title{%
    \textbf{
        Normalized Gaussian Processes
    }
}

\author{Rodrigo Luger}

\section{The problem}

Let $\mathbf{x} = \left( x_0 \,\, \cdots \,\, x_{K-1} \right)^\top$
be a $K$-dimensional multivariate normal random variable distributed
according to a Gaussian Process with mean $\pmb{\mu}$
and covariance $\pmb{\Sigma}$:
%
\begin{align}
    \mathbf{x} \sim \mathcal{N}\left( \pmb{\mu}, \pmb{\Sigma} \right)
    \quad.
\end{align}
%
For simplicity, let us assume the mean is constant, i.e.,
%
\begin{align}
    \pmb{\mu} = \mu \, \mathbf{j}
\end{align}
%
where $\mathbf{j}$ is the vector of $K$ ones.
%
Now suppose we cannot observe samples of $\mathbf{x}$ directly, but instead we can
observe samples from the \emph{normalized} process, which we will call
$\tilde{\mathbf{x}}$:
%
\begin{align}
    \tilde{\mathbf{x}}
     & \equiv \frac{\mathbf{x}}{\bar{x}}
    \nonumber                                                    \\[0.5em]
     & = \frac{\mathbf{x}}{\frac{1}{K}\sum\limits_{k=0}^{K-1} x}
    \quad.
\end{align}
%
The random variable $\tilde{\mathbf{x}}$ is no longer normally distributed, but as long as
the variance of $\bar{x}$ is small, we can approximate it as such. Let the mean
and covariance of $\tilde{\mathbf{x}}$ be $\tilde{\pmb{\mu}}$ and $\tilde{\pmb{\Sigma}}$, respectively:
%
\begin{align}
    \tilde{\mathbf{x}} \mathrel{\dot\sim} \mathcal{N}\left( \tilde{\pmb{\mu}}, \tilde{\pmb{\Sigma}} \right)
    \quad.
\end{align}
%
By definition, the mean is unity, i.e,
%
\begin{align}
    \tilde{\pmb{\mu}} = \mathbf{j}
    \quad.
\end{align}
%
What is the expression for $\tilde{\pmb{\Sigma}}$?

\section{The solution}

The covariance is given by
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \mathbb{E}\big[ (\tilde{\mathbf{x}} - \tilde{\pmb{\mu}}) (\tilde{\mathbf{x}} - \tilde{\pmb{\mu}} )^\top \big]
    \nonumber \\[0.5em]
     & =
    \mathbb{E}\left[
        \left(\frac{\mathbf{x}}{\bar{x}} - \mathbf{j}\right)
        \left(\frac{\mathbf{x}}{\bar{x}} - \mathbf{j}\right)^\top
        \right]
    \nonumber \\[0.5em]
     & =
    \mathbb{E}\left[
        \frac{\mathbf{x}\mathbf{x}^\top}{\bar{x}^2}
        -
        \frac{\mathbf{x}\,\mathbf{j}^\top}{\bar{x}}
        -
        \frac{\mathbf{j}\,\mathbf{x}^\top}{\bar{x}}
        +
        \mathbf{j} \, \mathbf{j}^\top
        \right]
    \nonumber \\[0.5em]
     & =
    \mathbb{E}\left[
        \frac{\mathbf{x}\mathbf{x}^\top}{\bar{x}^2}
        \right]
    -
    \mathbb{E}\left[
        \frac{\mathbf{x}\,\mathbf{j}^\top}{\bar{x}}
        \right]
    -
    \mathbb{E}\left[
        \frac{\mathbf{j}\,\mathbf{x}^\top}{\bar{x}}
        \right]
    +
    \mathbf{j} \, \mathbf{j}^\top
    \quad.
\end{align}
%
To evaluate this, it is convenient to write
%
\begin{align}
    \mathbf{x} = \pmb{\mu} + \mathbf{L} \mathbf{u}
    \quad,
\end{align}
%
where $\mathbf{L}$ is the lower Cholesky decomposition of $\pmb{\Sigma}$,
i.e.,
%
\begin{align}
    \pmb{\Sigma} = \mathbf{L}\,\mathbf{L}^\top
\end{align}
%
and $\mathbf{u}$ is a standard multivariate normal random variable,
%
\begin{align}
    \mathbf{u} \sim \mathcal{N}\left( 0, \mathbf{I} \right)
    \quad,
\end{align}
%
where $\mathbf{I}$ is the
$K \times K$ identity matrix.
The expression for the covariance $\tilde{\pmb{\Sigma}}$ may now be written
%
\begin{align}
    \label{eq:SigmaTildeExp}
    \tilde{\pmb{\Sigma}}
     & =
    \mathbb{E}\left[
        \frac{
            (\pmb{\mu} + \mathbf{L} \mathbf{u})(\pmb{\mu} + \mathbf{L} \mathbf{u})^\top
        }{
            \bar{x}^2
        }
        \right]
    -
    \mathbb{E}\left[
        \frac{
            (\pmb{\mu} + \mathbf{L} \mathbf{u})\mathbf{j}^\top
        }{
            \bar{x}
        }
        \right]
    -
    \mathbb{E}\left[
        \frac{
            \mathbf{j}(\pmb{\mu} + \mathbf{L} \mathbf{u})^\top
        }{
            \bar{x}
        }
        \right]
    +
    \mathbf{j} \, \mathbf{j}^\top
\end{align}
%
The mean $\bar{x}$ is
%
\begin{align}
    \bar{x} & = \sum\limits_{j=0}^{K-1}(\mu_j + L_{i,j}u_j)
    \nonumber                                               \\
            & =
    \mu + \frac{1}{K}\mathbf{j}^\top \mathbf{L} \, \mathbf{u}
    \nonumber                                               \\
            & = \mu(1 + z)
    \quad,
\end{align}
%
where we define the quantity
%
\begin{align}
    z \equiv \frac{1}{\mu K}\mathbf{j}^\top \mathbf{L} \, \mathbf{u}
    \quad.
\end{align}
%
Plugging this in and rearranging, we obtain
%
\begin{align}
    \label{eq:SigmaPQR}
    \tilde{\pmb{\Sigma}}
     & =
    \mathbf{P}
    -
    \frac{1}{\mu}
    \left(
    \mathbf{Q}
    +
    \mathbf{Q}^\top
    \right)
    +
    \frac{1}{\mu^2}
    \mathbf{R}
    \quad,
\end{align}
%
where we define the matrices
%
\begin{align}
    %
    \label{eq:Pexact}
    \mathbf{P} & \equiv
    \mathbb{E}\Big[
        \frac{z^2 \, \mathbf{j} \, \mathbf{j}^\top}{(1 + z)^2}
        \Big]
    %
    \\[0.5em]
    %
    \label{eq:Qexact}
    \mathbf{Q} & \equiv
    \mathbb{E}\left[
        \frac{z\, \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top}{(1 + z)^2}
        \right]
    %
    \\[0.5em]
    %
    \label{eq:Rexact}
    \mathbf{R} & \equiv
    \mathbb{E}\left[
        \frac{ \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \, \mathbf{L}^\top}{(1 + z)^2}
        \right]
\end{align}
%
The denominator in the expressions above makes direct evaluation of the expectations
intractable. Provided
$\big|z| < 1$ (an assumption we'll revisit below), we can Taylor expand the matrices as
%
\begin{align}
    \mathbf{P}
     & =
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^{n + 2}
        \,
        \mathbf{j} \, \mathbf{j}^\top
        \right]
    \\[0.5em]
    \mathbf{Q}
     & =
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^{n + 1} \,
        \mathbf{L}
        \,
        \mathbf{u}
        \,
        \mathbf{j}^\top
        \right]
    \\[0.5em]
    \mathbf{R}
     & =
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^n \, \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top
        \right]
\end{align}
%
In the Appendix, we show that the expecations in the expressions above may be computed from
%
\begin{align}
    \mathbb{E}\left[
        z^{n + 2} \, \mathbf{j} \, \mathbf{j}^\top
        \right]
     & =
    \frac{(n + 1) g_{n} \bar{\sigma}^\frac{n+2}{2}}{\mu^{n+2}} \, \mathbf{j} \, \mathbf{j}^\top
    \\[0.5em]
    \mathbb{E}\left[
        z^{n + 1} \, \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top
        \right]
     & =
    \frac{(n + 1) K \, g_{n} \bar{\sigma}^\frac{n}{2}}{\mu^{n+1}} \, \bar{\pmb{\sigma}} \, \mathbf{j}^\top
    \\[0.5em]
    \mathbb{E}\left[
        z^n \, \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top
        \right]
     & =
    \frac{g_n}{\mu^n} \left(
    n \,  \bar{\sigma}^\frac{n - 2}{2} \bar{\pmb{\sigma}} \, \bar{\pmb{\sigma}}^\top + \bar{\sigma}^\frac{n}{2} \pmb{\Sigma}
    \right)
\end{align}
%
where
%
\begin{align}
    g_n
     & \equiv
    \begin{cases}
        \dfrac{n!}{2^\frac{n}{2} \left(\frac{n}{2}\right)!} & n \, \mathrm{even}
        \\
        0                                                   & n \, \mathrm{odd}
        \quad,
    \end{cases}
\end{align}
%
is the expression for the $n^\mathrm{th}$ moment of the standard normal distribution,
%
\begin{align}
    \bar{\pmb{\sigma}}
     & \equiv
    \frac{1}{K} \pmb{\Sigma} \, \mathbf{j}
\end{align}
%
is the average of each row in $\pmb{\Sigma}$, and
%
\begin{align}
    \bar{\sigma}
     & \equiv
    \frac{1}{K^2} \mathbf{j}^\top \pmb{\Sigma} \, \mathbf{j}
\end{align}
%
is the average of all the elements in $\pmb{\Sigma}$.
%
Inserting these expressions into Equation~(\ref{eq:SigmaPQR}) and rearranging, we obtain
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \frac{\alpha \, \pmb{\Sigma}}{\mu^2}  +
    \frac{\alpha \, \left(\mathbf{s} \, \mathbf{s}^\top - \bar{\pmb{\sigma}} \, \bar{\pmb{\sigma}}^\top\right)}{\mu^2 \bar{\sigma}}  +
    \frac{\beta \, \left(\mathbf{s} \, \mathbf{s}^\top\right)}{\mu^2 \bar{\sigma}}
\end{align}
%
where
%
\begin{align}
    \mathbf{s}
     & \equiv
    \bar{\sigma} \, \mathbf{j} - \bar{\pmb{\sigma}}
\end{align}
%
and
%
\begin{align}
    \alpha
     & \equiv
    \sum\limits_{n=0}^N
    \frac{(2n + 1)!}{2^n \, n!}
    \left(
    \frac{\bar{\sigma}}{\mu^2}
    \right)^n
    \\[1em]
    \beta
     & \equiv
    \sum\limits_{n=0}^N
    \frac{2n(2n + 1)!}{2^n \, n!}
    \left(
    \frac{\bar{\sigma}}{\mu^2}
    \right)^n
\end{align}
%

\xxx{...}

This series is divergent!
This may be understood intuitively: given a (very) large number of draws from
a Gaussian process, one of the draws will eventually correspond to a value of
$\mathbf{x}$ whose
mean $\bar{x}$ is arbitrarily close to zero, for which the denominators in the
expectations in Equation~(\ref{eq:SigmaTildeExp}) diverge.

While this is true for any
$\mu$ and $\pmb{\Sigma} \ne \mathbf{0}$, in practice the probability of
drawing $\bar{x} \approx 0$ is vanishingly small provided that
%
\begin{align}
    \frac{\bar{\sigma}}{\mu^2} \ll 1
    \quad.
\end{align}
%
In this limit, any attempt to approximate $\tilde{\pmb{\Sigma}}$
numerically will yield a consistent, finite solution that
asymptotically approaches the expression derived above.

\ifdefined \includeAppendix

    \clearpage

    \appendix

    \section{Computing $\tilde{\pmb{\Sigma}}$}
    %
    Since the expectation is a linear operator, we can write Equation~(\ref{eq:SigmaTildeExp})
    as
    %
    \begin{align}
        \tilde{\pmb{\Sigma}}
         & =
        \frac{1}{\mu^2}
        \sum\limits_{n=0}^\infty
        (-1)^n (n + 1)
        \,
        \mathbb{E}\left[
            z^n
            \Bigg(
            \mu^2 \mathbf{j} \, \mathbf{j}^\top
            +
            \mu \, \mathbf{j}\mathbf{u}^\top \mathbf{L}^\top
            +
            \mu \, \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top
            +
            \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \, \mathbf{L}^\top
            \Bigg)
            \right]
        - \mathbf{j} \, \mathbf{j}^\top
        \nonumber \\[0.5em]
         & =
        \sum\limits_{n=0}^\infty
        \frac{(-1)^n}{\mu^{2 + n}}
        \,
        \left(
        \mu^2 \mathbf{P}_n
        +
        \mu \, \mathbf{Q}_n
        +
        \mu \, \mathbf{Q}_n^\top
        +
        \mathbf{R}_n
        \right)
        - \mathbf{j} \, \mathbf{j}^\top
        \quad,
    \end{align}
    %
    where we define the matrices
    %
    \begin{align}
        \mathbf{P}_n & \equiv \frac{(n + 1)}{K^n}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^n \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                \\[0.5em]
        \mathbf{Q}_n & \equiv \frac{(n + 1)}{K^n}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^n \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                \\[0.5em]
        \mathbf{R}_n & \equiv \frac{(n + 1)}{K^n}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^n \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \, \mathbf{L}^\top \right]
        \quad.
    \end{align}
    %
    In order to compute these matrices,
    we will make use of the expression for the
    $n^\mathrm{th}$ moment of the standard normal distribution:
    %
    \begin{align}
        \mathbb{E}\left[ u^n \right] & = g_n
    \end{align}
    %
    for scalar $u \sim \mathcal{N}(0, 1)$, where
    %
    \begin{align}
        g_n
         & =
        \begin{cases}
            \dfrac{n!}{2^\frac{n}{2} \left(\frac{n}{2}\right)!} & n \, \mathrm{even}
            \\
            0                                                   & n \, \mathrm{odd}
            \quad.
        \end{cases}
    \end{align}
    %
    \citep[e.g.,][]{Winkelbauer2012}.
    We can apply this relation to compute expectations
    of products of the components of a random standard normal vector $\mathbf{u}$:
    %
    \begin{align}
        \mathbb{E}\left[u_i \right]                  & = 0
        \\[1em]
        \mathbb{E}\left[u_i u_j\right]               & = g_2
        \\[1em]
        \mathbb{E}\left[u_i u_j u\right]             & = 0
        \\[1em]
        \mathbb{E}\left[u_i u_j u u_l\right]         & =
        g_4 \, \delta_{i, j} \delta_{k, l} \, \delta_{i, k}
        +
        g_2^2 \left(
        \delta_{i, j} \, \delta_{k, l} \, \bar{\delta}_{i, k}
        +
        \delta_{i, k} \, \delta_{j, l} \, \bar{\delta}_{i, j}
        +
        \delta_{i, l} \, \delta_{j, k} \, \bar{\delta}_{i, j}
        \right)
        \\[1em]
        \mathbb{E}\left[u_i u_j u u_l u_m\right]     & = 0
        \\[1em]
        \mathbb{E}\left[u_i u_j u u_l u_m u_n\right] & =
        ...
        \quad,
    \end{align}
    %
    where $\delta$ is the Kronecker delta and $\bar{\delta} \equiv 1 - \delta$.

    \subsection{The $\mathbf{P}$ expectation}

    Let us begin by computing the first several terms in $\mathbf{P}$.
    %
    \begin{align}
        \mathbf{P}_0 & = \mathbb{E}\left[ \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                               \\
                     & = \mathbf{j} \, \mathbf{j}^\top
    \end{align}
    %
    since $\mathbf{j} \, \mathbf{j}^\top$ is constant. Next,
    %
    \begin{align}
        \mathbf{P}_1 & = \frac{2}{K}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                \\
                     & = \frac{2}{K}\left(\mathbf{j}^\top \mathbf{L} \, \mathbb{E}\left[ \mathbf{u} \right]\right) \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                \\
                     & = \mathbf{0}
    \end{align}
    %
    since $\mathbb{E}\left[ \mathbf{u} \right] = \mathbf{0}$. The next term is
    %
    \begin{align}
        \mathbf{P}_2 & = \frac{3}{K^2}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^2 \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                   \\
                     & = \frac{3}{K^2}\mathbb{E}\left[\left(\mathbf{j}^\top \mathbf{L} \,  \mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j} \right)  \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                   \\
                     & = \frac{3}{K^2}\left(\mathbf{j}^\top \mathbf{L} \, \mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top \right] \mathbf{L}^\top \, \mathbf{j} \right)  \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                                                                   \\
                     & = \frac{3}{K^2}\left(\mathbf{j}^\top \mathbf{L} \mathbf{L}^\top \, \mathbf{j} \right)  \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                                                                   \\
                     & = \frac{3}{K^2}\mathrm{sum}(\mathbf{\Sigma}) \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                                                                   \\
                     & = 3 \, \bar{\sigma} \, \mathbf{j} \, \mathbf{j}^\top
    \end{align}
    %
    where $\bar{\sigma}$ is the average of all entries in $\pmb{\Sigma}$ and
    we made use of the fact that $\mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top\right] = \mathbf{I}$.
    %
    Next,
    %
    \begin{align}
        \mathbf{P}_3 & = \frac{4}{K^3}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^3 \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                                                               \\
                     & = \frac{4}{K^3}\mathbb{E}\left[\left(\mathbf{j}^\top \mathbf{L} \,  \mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \mathbf{L} \,  \mathbf{u} \right) \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                                                               \\
                     & = \frac{4}{K^3}\left(\mathbf{j}^\top \mathbf{L} \,  \mathbb{E}\left[\mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \mathbf{L} \,  \mathbf{u}\right] \right) \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                                                                                                               \\
                     & = \mathbf{0}
    \end{align}
    %
    where we made use of the fact that we may write the component at index $i$ of the vector within the expectation in the next-to-last line
    as
    \begin{align}
        \mathbb{E}\left[\mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \mathbf{L} \,  \mathbf{u}\right]_{i} = \sum\limits_{k,l}(\mathbf{L}^\top \mathbf{j} \, \mathbf{j}^\top \mathbf{L})_{kl} u_i u u_l = 0
        \quad,
        \nonumber
    \end{align}
    %
    which is zero since it is the product of an odd number of random normal variables.
    Finally,
    %
    \begin{align}
        \mathbf{P}_4 & = \frac{5}{K^4}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^4 \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                                                                                                                 \\
                     & = \frac{5}{K^4}\mathbb{E}\left[\left(\mathbf{j}^\top \mathbf{L} \,  \mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \mathbf{L} \,  \mathbf{u} \, \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j} \right) \mathbf{j} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                                                                                                                 \\
                     & = \frac{5}{K^4}\left(\mathbf{j}^\top \mathbf{L} \, \mathbf{E} \, \mathbf{L}^\top \, \mathbf{j} \right) \mathbf{j} \, \mathbf{j}^\top
        \quad,
    \end{align}
    %
    where we define
    %
    \begin{align}
        \mathbf{E} \equiv \mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top  \mathbf{A} \,  \mathbf{u} \mathbf{u}^\top \right]
    \end{align}
    %
    and
    %
    \begin{align}
        \mathbf{A} \equiv \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \, \mathbf{L}
        \quad.
    \end{align}
    %
    The components of this matrix are
    %
    \begin{align}
        E_{ij} & =
        \sum\limits_{k,l}A_{kl} \mathbb{E}(u_i u_j u u_l)
        \nonumber  \\
               & =
        \sum\limits_{k,l}A_{kl}
        \Big(
        g_4 \, \delta_{i, j} \, \delta_{k, l} \, \delta_{i, k}
        +
        g_2^2 \big(
            \delta_{i, j} \, \delta_{k, l} \, \bar{\delta}_{i, k}
            +
            \delta_{i, k} \, \delta_{j, l} \, \bar{\delta}_{i, j}
            +
            \delta_{i, l} \, \delta_{j, k} \, \bar{\delta}_{i, j}
            \big)
        \Big)
        \nonumber  \\
               & =
        2 A_{ij} + \sum\limits A_{kk}
        \quad,
    \end{align}
    %
    where we used the fact that $\mathbf{A} = \mathbf{A}^\top$. We may thus write
    %
    \begin{align}
        \mathbf{E} & =
        2 \, \mathbf{A} + \mathrm{tr}(\mathbf{A}) \mathbf{I}
        \nonumber      \\
                   & =
        2 \, \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \, \mathbf{L} + \mathrm{sum}(\pmb{\Sigma}) \mathbf{I}
        \quad.
    \end{align}
    %
    Inserting this back into the expression for $\mathbf{P}_4$, we obtain
    %
    \begin{align}
        \mathbf{P}_4 & =
        \frac{5}{K^4}
        \bigg(
        2 \, \big(\mathbf{j}^\top \mathbf{L} \, \mathbf{L}^\top \, \mathbf{j} \, \mathbf{j}^\top \, \mathbf{L} \, \mathbf{L}^\top \, \mathbf{j} \big)
        +
        \mathrm{sum}(\pmb{\Sigma}) \big(\mathbf{j}^\top \mathbf{L} \, \mathbf{L}^\top \, \mathbf{j} \big)
        \bigg) \mathbf{j} \, \mathbf{j}^\top
        \nonumber        \\
                     & =
        3 \, \bar{\sigma}^2 \mathbf{j} \, \mathbf{j}^\top
        \quad.
    \end{align}
    %
    The general term in this sequence is
    %
    \begin{align}
        \mathbf{P}_n & =
        \mathbf{j} \, \mathbf{j}^\top
        \begin{cases}
            0                                       & n \, \mathrm{odd}
            \\
            g_n (n + 1) \, \bar{\sigma}^\frac{n}{2} & n \, \mathrm{even}
            \quad.
        \end{cases}
    \end{align}
    %


    \subsection{The $\mathbf{Q}$ expectation}
    %
    Let us again compute the first several terms of this expectation matrix.
    %
    \begin{align}
        \mathbf{Q}_0 & = \mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                              \\
                     & = \mathbf{L} \, \mathbb{E}\left[  \mathbf{u} \right] \, \mathbf{j}^\top
        \nonumber                                                                                                                                              \\
                     & = \mathbf{0}
        %
        \\[1em]
        %
        \mathbf{Q}_1 & = \frac{1}{K}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                              \\
                     & = \frac{1}{K}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{L} \, \mathbf{u} \right] \, \mathbf{j}^\top
        \nonumber                                                                                                                                              \\
                     & = \frac{1}{K}\mathbb{E}\left[ \mathbf{L} \, \mathbf{u} (\mathbf{u}^\top \mathbf{L}^\top \mathbf{j}) \right] \, \mathbf{j}^\top
        \nonumber                                                                                                                                              \\
                     & = \frac{1}{K}\mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \mathbf{u}^\top \mathbf{L}^\top  \right] \, \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                              \\
                     & = \frac{1}{K}\mathbf{L} \, \mathbb{E}\left[  \mathbf{u} \mathbf{u}^\top   \right]  \, \mathbf{L}^\top \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                              \\
                     & = \frac{1}{K}\mathbf{L} \, \mathbf{L}^\top \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                              \\
                     & = \frac{1}{K}\pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top
    \end{align}
    %
    where we made liberal use of the fact that $\left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)$ is a scalar. Continuing,
    %
    \begin{align}
        \mathbf{Q}_2 & = \frac{2}{K^2}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^2 \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                    \\
                     & =  \frac{2}{K^2}\mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \, (\mathbf{j}^\top \mathbf{L} \, \mathbf{u}) (\mathbf{u}^\top \mathbf{L}^\top \mathbf{j}) \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                                                    \\
                     & = \frac{2}{K^2}\mathbf{L} \, \mathbb{E}\left[ \mathbf{u} \, \mathbf{j}^\top \mathbf{L} \, \mathbf{u} \mathbf{u}^\top \right] \, \mathbf{L}^\top \mathbf{j} \, \mathbf{j}^\top
        \nonumber                                                                                                                                                                                    \\
                     & = \mathbf{0}
    \end{align}
    %
    since again the components of the expectation in the next-to-last line involve products of three standard normal random variables.
    Next,
    \begin{align}
        \mathbf{Q}_3 & = \frac{3}{K^3}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^3 \mathbf{L} \, \mathbf{u} \, \mathbf{j}^\top \right]
        \nonumber                                                                                                                                                  \\
                     & = \xxx{...}
        \nonumber                                                                                                                                                  \\
                     & =
        \frac{3}{K^3}\mathbf{e} \, \mathbf{j}^\top
    \end{align}
    %
    % 3 \, \pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top \, \pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top
    %
    where we define
    %
    \begin{align}
        \mathbf{e} \equiv \mathbb{E}\left[
            \left(
            \mathbf{v}^\top \mathbf{u} \mathbf{u}^\top \mathbf{v}
            \mathbf{v}^\top \mathbf{u}
            \right)
            \mathbf{L} \mathbf{u}
            \right]
    \end{align}
    %
    and
    %
    \begin{align}
        \mathbf{v} \equiv \mathbf{L}^\top \, \mathbf{j}
        \quad.
    \end{align}
    %
    The components of $\mathbf{e}$ are given by
    %
    \begin{align}
        e_{i} & =
        \sum\limits_{j,k,l,m}L_{ij} v v_l v_m \mathbb{E}(u_j u u_l u_m)
        \nonumber \\
              & =
        \sum\limits_{j,k,l,m}L_{ij} v v_l v_m
        \Big(
        g_4 \, \delta_{j, k} \, \delta_{l, m} \, \delta_{j, l}
        +
        g_2^2 \big(
            \delta_{j, k} \, \delta_{l, m} \, \bar{\delta}_{j, l}
            +
            \delta_{j, l} \, \delta_{k, m} \, \bar{\delta}_{j, k}
            +
            \delta_{j, m} \, \delta_{k, l} \, \bar{\delta}_{j, k}
            \big)
        \Big)
        \nonumber \\
              & =
        \xxx{...}
    \end{align}
    %
    We may thus write
    %
    \begin{align}
        \mathbf{e} & = 3 \, \pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top \, \pmb{\Sigma} \, \mathbf{j}
        \quad.
    \end{align}
    %
    Inserting this back into the expression for $\mathbf{Q}_3$, we obtain
    %
    \begin{align}
        \mathbf{Q}_3 & =
        \frac{9}{K^3} \, (\pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top)^2
    \end{align}
    %
    Based on this pattern, the general term in $\mathbf{Q}$ is
    %
    \begin{align}
        \mathbf{Q}_n & =
        \begin{cases}
            \mathbf{0}                                                                                    & n \, \mathrm{even}
            \\
            \frac{(n + 1)g_{n+1}}{K^n} \, (\pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top)^\frac{n + 1}{2} & n \, \mathrm{odd}
            \quad.
        \end{cases}
    \end{align}

    \subsection{The $\mathbf{R}$ expectation}
    %
    The first several terms in $\mathbf{R}$ are given by
    %
    \setlength{\abovedisplayskip}{1em}
    \begin{align}
        \mathbf{R}_0 & = \mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
        \nonumber                                                                                                                                                              \\
                     & = \mathbf{L} \, \mathbb{E}\left[  \mathbf{u} \, \mathbf{u}^\top \right] \, \mathbf{L}^\top
        \nonumber                                                                                                                                                              \\
                     & = \pmb{\Sigma}
        %
        \\[1em]
        %
        \mathbf{R}_1 & = \frac{1}{K}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
        \nonumber                                                                                                                                                              \\
                     & = \mathbf{0}
        %
    \end{align}
    %
    since its elements are all products of three standard normal random variables. Next,
    %
    \begin{align}
        %
        \mathbf{R}_2 & = \mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^2 \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
        \nonumber                                                                                                                                                     \\
                     & = \xxx{...}
        \nonumber                                                                                                                                                     \\
                     & = 2 \pmb{\Sigma} \mathbf{j} \, \mathbf{j}^\top \pmb{\Sigma} + \mathrm{sum}(\pmb{\Sigma}) \pmb{\Sigma}
    \end{align}
    %
    \xxx{By trial and error, I found that}
    %
    \begin{align}
        %
        \mathbf{R}_4 & = \frac{5}{K^4}\mathbb{E}\left[ \left(\mathbf{j}^\top \mathbf{L} \, \mathbf{u}\right)^4 \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
        \nonumber                                                                                                                                                                  \\
                     & = \xxx{...}
        \nonumber                                                                                                                                                                  \\
                     & = \frac{5}{K^4} \Big( 12 (\pmb{\Sigma} \mathbf{j} \, \mathbf{j}^\top)^2 \pmb{\Sigma} + 3 \, \mathrm{sum}(\pmb{\Sigma})^2 \pmb{\Sigma} \Big)
    \end{align}
    %
    Based on this pattern, the general term in $\mathbf{R}$ is
    %
    \begin{align}
        \mathbf{R}_n & =
        \begin{cases}
            \mathbf{0}                                                                                                                                                      & n \, \mathrm{odd}
            \\
            \frac{n (n + 1) g_{n}}{K^n} \, (\pmb{\Sigma} \, \mathbf{j} \, \mathbf{j}^\top)^\frac{n}{2}\pmb{\Sigma} + (n + 1) g_{n} \, \bar{\sigma}^\frac{n}{2} \pmb{\Sigma} & n \, \mathrm{even}
            \quad.
        \end{cases}
    \end{align}

    \subsection{The final result}
    Inserting the matrices $\mathbf{P}$, $\mathbf{Q}$, and $\mathbf{R}$ into the expression
    for $\tilde{\pmb{\Sigma}}$ and rearranging,
    we obtain
    %
    \begin{align}
        \tilde{\pmb{\Sigma}}
         & =
        \frac{1}{\mu^2} \pmb{\Sigma}
        +
        \frac{1}{\mu^2}
        \sum\limits_{n=1}^\infty
        \frac{(-1)^n(n + 1)}{\mu^{n}}
        \,
        \bigg[
            g_n \, \bar{\sigma}^\frac{n}{2} \, (\pmb{\Sigma} + \mu^2\mathbf{j} \, \mathbf{j}^\top)
            \, +
            \nonumber                 \\[0.5em]
         & \phantom{XXXXXXXXXXXXXXX.}
        K g_{n+1} \mu  \,
        \left(
        \left(\pmb{\Sigma} \, \frac{\mathbf{j} \, \mathbf{j}^\top}{K^2}\right)^\frac{n + 1}{2}
        +
        \left(\frac{\mathbf{j} \, \mathbf{j}^\top}{K^2} \, \pmb{\Sigma}\right)^\frac{n + 1}{2}
        \right)
        \, +
        \nonumber                     \\[0.5em]
         & \phantom{XXXXXXXXXXXXXXX.}
        n g_n \, \left(\pmb{\Sigma} \, \frac{\mathbf{j} \, \mathbf{j}^\top}{K^2}\right)^\frac{n}{2}\pmb{\Sigma}
        \bigg]
        \quad.
    \end{align}

\else

    \clearpage

    \appendix

    See \citet{Winkelbauer2012}...

\fi

\bibliography{bib}

\end{document}