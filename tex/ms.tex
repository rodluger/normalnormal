\documentclass[modern]{aastex62}

% Load the corTeX style definitions
\input{cortex}

% Load custom style
\input{style}


% Bibliography
\bibliographystyle{aasjournal}

\usepackage{etoolbox}
\makeatletter % we need to patch \env@cases that has @ in its name
\patchcmd{\env@cases}{\quad}{\qquad\qquad}{}{}
\makeatother

\usepackage{enumitem}

% Begin!
\begin{document}

% Title
\title{%
    \textbf{
        Normalized Gaussian Processes
    }
}

\author{Rodrigo Luger}

\section{The problem}

Let $\mathbf{x} = \left( x_0 \,\, \cdots \,\, x_{K-1} \right)^\top$
be a $K$-dimensional multivariate normal random variable distributed
according to a Gaussian Process with mean $\pmb{\mu}$
and covariance $\pmb{\Sigma}$:
%
\begin{align}
    \mathbf{x} \sim \mathcal{N}\left( \pmb{\mu}, \pmb{\Sigma} \right)
    \quad.
\end{align}
%
For simplicity, let us assume the mean is constant, i.e.,
%
\begin{align}
    \pmb{\mu} = \mu \, \mathbf{j}_K
\end{align}
%
where $\mathbf{j}_K$ is the vector of $K$ ones.
%
Now suppose we cannot observe samples of $\mathbf{x}$ directly, but instead we can
observe samples from the \emph{normalized} process, which we will call
$\tilde{\mathbf{x}}$:
%
\begin{align}
    \tilde{\mathbf{x}}
     & \equiv \frac{\mathbf{x}}{\bar{x}}
    \nonumber                                                      \\[0.5em]
     & = \frac{\mathbf{x}}{\frac{1}{K}\sum\limits_{k=0}^{K-1} x_k}
    \quad.
\end{align}
%
The random variable $\tilde{\mathbf{x}}$ is no longer normally distributed, but as long as
the variance of $\bar{x}$ is small, we can approximate it as such. Let the mean
and covariance of $\tilde{\mathbf{x}}$ be $\tilde{\pmb{\mu}}$ and $\tilde{\pmb{\Sigma}}$, respectively:
%
\begin{align}
    \tilde{\mathbf{x}} \mathrel{\dot\sim} \mathcal{N}\left( \tilde{\pmb{\mu}}, \tilde{\pmb{\Sigma}} \right)
    \quad.
\end{align}
%
By definition, the mean is unity, i.e,
%
\begin{align}
    \tilde{\pmb{\mu}} = \mathbf{j}_K
    \quad.
\end{align}
%
What is the expression for $\tilde{\pmb{\Sigma}}$?

\section{The solution}

The covariance is given by
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \mathbb{E}\big[ (\tilde{\mathbf{x}} - \tilde{\pmb{\mu}}) (\tilde{\mathbf{x}} - \tilde{\pmb{\mu}} )^\top \big]
    \nonumber \\[0.5em]
     & =
    \mathbb{E}\left[
        \left(\frac{\mathbf{x}}{\bar{x}} - \mathbf{j}_K\right)
        \left(\frac{\mathbf{x}}{\bar{x}} - \mathbf{j}_K\right)^\top
        \right]
    \nonumber \\[0.5em]
     & =
    \mathbb{E}\left[
        \frac{\mathbf{x}\mathbf{x}^\top}{\bar{x}^2}
        -
        \frac{\mathbf{x}\,\mathbf{j}_K^\top}{\bar{x}}
        -
        \frac{\mathbf{j}\,\mathbf{x}_K^\top}{\bar{x}}
        +
        \mathbf{J}_K
        \right]
    \nonumber \\[0.5em]
     & =
    \mathbb{E}\left[
        \frac{\mathbf{x}\mathbf{x}^\top}{\bar{x}^2}
        \right]
    -
    \mathbb{E}\left[
        \frac{\mathbf{x}\,\mathbf{j}_K^\top}{\bar{x}}
        \right]
    -
    \mathbb{E}\left[
        \frac{\mathbf{j}_K\,\mathbf{x}^\top}{\bar{x}}
        \right]
    +
    \mathbf{J}_K
    \quad,
\end{align}
%
where $\mathbf{J}_K = \mathbf{j}_K \mathbf{j}_K^\top$ is the $K\times K$ matrix of ones.
To evaluate this, it is convenient to write
%
\begin{align}
    \mathbf{x} = \pmb{\mu} + \mathbf{L} \mathbf{u}
    \quad,
\end{align}
%
where $\mathbf{L}$ is the lower Cholesky decomposition of $\pmb{\Sigma}$,
i.e.,
%
\begin{align}
    \pmb{\Sigma} = \mathbf{L}\,\mathbf{L}^\top
\end{align}
%
and $\mathbf{u}$ is a standard multivariate normal random variable,
%
\begin{align}
    \mathbf{u} \sim \mathcal{N}\left( 0, \mathbf{I}_K \right)
    \quad,
\end{align}
%
where $\mathbf{I}_K$ is the
$K \times K$ identity matrix.
The expression for the covariance $\tilde{\pmb{\Sigma}}$ may now be written
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \mathbb{E}\left[
        \frac{
            (\pmb{\mu} + \mathbf{L} \mathbf{u})(\pmb{\mu} + \mathbf{L} \mathbf{u})^\top
        }{
            \bar{x}^2
        }
        \right]
    -
    \mathbb{E}\left[
        \frac{
            (\pmb{\mu} + \mathbf{L} \mathbf{u})\mathbf{j}_K^\top
        }{
            \bar{x}
        }
        \right]
    -
    \mathbb{E}\left[
        \frac{
            \mathbf{j}_K(\pmb{\mu} + \mathbf{L} \mathbf{u})^\top
        }{
            \bar{x}
        }
        \right]
    +
    \mathbf{J}_K
\end{align}
%
The mean $\bar{x}$ is
%
\begin{align}
    \bar{x} & = \sum\limits_{j=0}^{K-1}(\mu_j + L_{i,j}u_j)
    \nonumber                                               \\
            & =
    \mu + \frac{1}{K}\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}
    \nonumber                                               \\
            & = \mu(1 + z)
    \quad,
\end{align}
%
where we define the quantity
%
\begin{align}
    z \equiv \frac{1}{\mu K}\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}
    \quad.
\end{align}
%
Plugging this in and rearranging, we obtain
%
\begin{align}
    \label{eq:SigmaPQR}
    \tilde{\pmb{\Sigma}}
     & =
    \mathbf{P}
    -
    \frac{1}{\mu}
    \left(
    \mathbf{Q}
    +
    \mathbf{Q}^\top
    \right)
    +
    \frac{1}{\mu^2}
    \mathbf{R}
    \quad,
\end{align}
%
where we define the matrices
%
\begin{align}
    %
    \label{eq:Pexact}
    \mathbf{P} & \equiv
    \mathbb{E}\Big[
        \frac{z^2 \, \mathbf{J}_K}{(1 + z)^2}
        \Big]
    %
    \\[0.5em]
    %
    \label{eq:Qexact}
    \mathbf{Q} & \equiv
    \mathbb{E}\left[
        \frac{z\, \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top}{(1 + z)^2}
        \right]
    %
    \\[0.5em]
    %
    \label{eq:Rexact}
    \mathbf{R} & \equiv
    \mathbb{E}\left[
        \frac{ \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \, \mathbf{L}^\top}{(1 + z)^2}
        \right]
\end{align}
%
The denominator in the expressions above makes direct evaluation of the expectations
intractable. As long as
$\big|z| < 1$, we can Taylor expand the matrices as
%
\begin{align}
    \mathbf{P}
     & =
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^{n + 2}
        \,
        \mathbf{J}_K
        \right]
    \\[0.5em]
    \mathbf{Q}
     & =
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^{n + 1} \,
        \mathbf{L}
        \,
        \mathbf{u}
        \,
        \mathbf{j}_K^\top
        \right]
    \\[0.5em]
    \mathbf{R}
     & =
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^n \, \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top
        \right]
\end{align}
%
In the Appendix, we show that these may be computed from
%
\begin{align}
    \mathbb{E}\left[
        z^{n + 2} \, \mathbf{J}_K
        \right]
     & =
    \frac{(n + 1) g_{n}}{\mu^{n+2}} \, \bar{\Sigma}^\frac{n+2}{2} \, \mathbf{J}_K
    \\[0.5em]
    \mathbb{E}\left[
        z^{n + 1} \, \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top
        \right]
     & =
    \frac{(n + 1) K \, g_{n}}{\mu^{n+1}} \, \mathbf{S}^\frac{n+2}{2}
    \\[0.5em]
    \mathbb{E}\left[
        z^n \, \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top
        \right]
     & =
    \frac{g_n}{\mu^n} \left(
    n \, \mathbf{S}^\frac{n}{2}\pmb{\Sigma} + \bar{\Sigma}^\frac{n}{2} \pmb{\Sigma}
    \right)
\end{align}
%
where we define
%
\begin{align}
    g_n
     & \equiv
    \begin{cases}
        \dfrac{n!}{2^\frac{n}{2} \left(\frac{n}{2}\right)!} & n \, \mathrm{even}
        \\
        0                                                   & n \, \mathrm{odd}
        \quad,
    \end{cases}
\end{align}
%
\begin{align}
    \mathbf{S}
     & \equiv
    \frac{1}{K^2} \pmb{\Sigma} \, \mathbf{J}_K
    \quad,
\end{align}
%
and $\bar{\Sigma}$ is the average of all the elements in $\pmb{\Sigma}$.
%
Inserting these expressions into Equation~(\ref{eq:SigmaPQR}), we obtain
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \resizebox{.85\hsize}{!}{$
            \frac{1}{\mu^2}
            \sum\limits_{n=0}^\infty
            \frac{(-1)^n (n + 1)^2 g_n}{\mu^n}
            \left(
            \bar{\Sigma}^\frac{n+2}{2} \mathbf{J}_K
            -
            K \left( \mathbf{S}^\frac{n+2}{2} + {\mathbf{S}^\top}^\frac{n+2}{2} \right)
            +
            \frac{n}{(n+1)}
            \left(
            \mathbf{S}^\frac{n}{2}
            +
            \bar{\Sigma}^\frac{n}{2} \mathbf{I}_K
            \right)
            \mathbf{\Sigma}
            \right)
        $}
    \quad.
\end{align}
%
%
The convergence criterion for this series may be written
%
\begin{align}
    \mathbb{E}\big[|z|\big] < 1
\end{align}
%
or, equivalently,
%
\begin{align}
    \mathbb{E}\big[z^2\big]    & < 1
    \nonumber                        \\
    \frac{\bar{\Sigma}}{\mu^2} & < 1
    \quad.
\end{align}
%

\clearpage

\appendix

\xxx{THIS ALL NEEDS TO BE REWRITTEN!}

\section{Computing $\tilde{\pmb{\Sigma}}$}
%
Since the expectation is a linear operator, we can write Equation~(\ref{eq:SigmaTildeExp})
as
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \frac{1}{\mu^2}
    \sum\limits_{n=0}^\infty
    (-1)^n (n + 1)
    \,
    \mathbb{E}\left[
        z^n
        \Bigg(
        \mu^2 \mathbf{J}_K
        +
        \mu \, \mathbf{j}_K\mathbf{u}^\top \mathbf{L}^\top
        +
        \mu \, \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top
        +
        \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \, \mathbf{L}^\top
        \Bigg)
        \right]
    - \mathbf{J}_K
    \nonumber \\[0.5em]
     & =
    \sum\limits_{n=0}^\infty
    \frac{(-1)^n}{\mu^{2 + n}}
    \,
    \left(
    \mu^2 \mathbf{P}_n
    +
    \mu \, \mathbf{Q}_n
    +
    \mu \, \mathbf{Q}_n^\top
    +
    \mathbf{R}_n
    \right)
    - \mathbf{J}_K
    \quad,
\end{align}
%
where we define the matrices
%
\begin{align}
    \mathbf{P}_n & \equiv \frac{(n + 1)}{K^n}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^n \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                                  \\[0.5em]
    \mathbf{Q}_n & \equiv \frac{(n + 1)}{K^n}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^n \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top \right]
    \nonumber                                                                                                                                                                                  \\[0.5em]
    \mathbf{R}_n & \equiv \frac{(n + 1)}{K^n}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^n \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \, \mathbf{L}^\top \right]
    \quad.
\end{align}
%
In order to compute these matrices,
we will make use of the expression for the
$n^\mathrm{th}$ moment of the standard normal distribution:
%
\begin{align}
    \mathbb{E}\left[ u^n \right] & = g_n
\end{align}
%
for scalar $u \sim \mathcal{N}(0, 1)$, where
%
\begin{align}
    g_n
     & =
    \begin{cases}
        \dfrac{n!}{2^\frac{n}{2} \left(\frac{n}{2}\right)!} & n \, \mathrm{even}
        \\
        0                                                   & n \, \mathrm{odd}
        \quad.
    \end{cases}
\end{align}
%
\citep[e.g.,][]{Winkelbauer2012}.
We can apply this relation to compute expectations
of products of the components of a random standard normal vector $\mathbf{u}$:
%
\begin{align}
    \mathbb{E}\left[u_i \right]                    & = 0
    \\[1em]
    \mathbb{E}\left[u_i u_j\right]                 & = g_2
    \\[1em]
    \mathbb{E}\left[u_i u_j u_k\right]             & = 0
    \\[1em]
    \mathbb{E}\left[u_i u_j u_k u_l\right]         & =
    g_4 \, \delta_{i, j} \delta_{k, l} \, \delta_{i, k}
    +
    g_2^2 \left(
    \delta_{i, j} \, \delta_{k, l} \, \bar{\delta}_{i, k}
    +
    \delta_{i, k} \, \delta_{j, l} \, \bar{\delta}_{i, j}
    +
    \delta_{i, l} \, \delta_{j, k} \, \bar{\delta}_{i, j}
    \right)
    \\[1em]
    \mathbb{E}\left[u_i u_j u_k u_l u_m\right]     & = 0
    \\[1em]
    \mathbb{E}\left[u_i u_j u_k u_l u_m u_n\right] & =
    ...
    \quad,
\end{align}
%
where $\delta$ is the Kronecker delta and $\bar{\delta} \equiv 1 - \delta$.

\subsection{The $\mathbf{P}$ expectation}

Let us begin by computing the first several terms in $\mathbf{P}$.
%
\begin{align}
    \mathbf{P}_0 & = \mathbb{E}\left[ \mathbf{J}_K \right]
    \nonumber                                              \\
                 & = \mathbf{J}_K
\end{align}
%
since $\mathbf{J}_K$ is constant. Next,
%
\begin{align}
    \mathbf{P}_1 & = \frac{2}{K}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{J}_K \right]
    \nonumber                                                                                                                 \\
                 & = \frac{2}{K}\left(\mathbf{j}_K^\top \mathbf{L} \, \mathbb{E}\left[ \mathbf{u} \right]\right) \mathbf{J}_K
    \nonumber                                                                                                                 \\
                 & = \mathbf{0}
\end{align}
%
since $\mathbb{E}\left[ \mathbf{u} \right] = \mathbf{0}$. The next term is
%
\begin{align}
    \mathbf{P}_2 & = \frac{3}{K^2}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^2 \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                      \\
                 & = \frac{3}{K^2}\mathbb{E}\left[\left(\mathbf{j}_K^\top \mathbf{L} \,  \mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j}_K \right)  \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                      \\
                 & = \frac{3}{K^2}\left(\mathbf{j}_K^\top \mathbf{L} \, \mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top \right] \mathbf{L}^\top \, \mathbf{j}_K \right)  \mathbf{J}_K
    \nonumber                                                                                                                                                                      \\
                 & = \frac{3}{K^2}\left(\mathbf{j}_K^\top \mathbf{L} \mathbf{L}^\top \, \mathbf{j}_K \right)  \mathbf{J}_K
    \nonumber                                                                                                                                                                      \\
                 & = \frac{3}{K^2}\mathrm{sum}(\mathbf{\Sigma}) \mathbf{J}_K
    \nonumber                                                                                                                                                                      \\
                 & = 3 \, \bar{\Sigma} \, \mathbf{J}_K
\end{align}
%
where $\bar{\Sigma}$ is the average of all entries in $\pmb{\Sigma}$ and
we made use of the fact that $\mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top\right] = \mathbf{I}_K$.
%
Next,
%
\begin{align}
    \mathbf{P}_3 & = \frac{4}{K^3}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^3 \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                                                                    \\
                 & = \frac{4}{K^3}\mathbb{E}\left[\left(\mathbf{j}_K^\top \mathbf{L} \,  \mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j}_K \, \mathbf{j}_K^\top \mathbf{L} \,  \mathbf{u} \right) \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                                                                    \\
                 & = \frac{4}{K^3}\left(\mathbf{j}_K^\top \mathbf{L} \,  \mathbb{E}\left[\mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{J}_K \mathbf{L} \,  \mathbf{u}\right] \right) \mathbf{J}_K
    \nonumber                                                                                                                                                                                                                    \\
                 & = \mathbf{0}
\end{align}
%
where we made use of the fact that we may write the component at index $i$ of the vector within the expectation in the next-to-last line
as
\begin{align}
    \mathbb{E}\left[\mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{J}_K \mathbf{L} \,  \mathbf{u}\right]_{i} = \sum\limits_{k,l}(\mathbf{L}^\top \mathbf{J}_K \mathbf{L})_{kl} u_i u_k u_l = 0
    \quad,
    \nonumber
\end{align}
%
which is zero since it is the product of an odd number of random normal variables.
Finally,
%
\begin{align}
    \mathbf{P}_4 & = \frac{5}{K^4}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^4 \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                                                                                                                        \\
                 & = \frac{5}{K^4}\mathbb{E}\left[\left(\mathbf{j}_K^\top \mathbf{L} \,  \mathbf{u} \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j}_K \, \mathbf{j}_K^\top \mathbf{L} \,  \mathbf{u} \, \mathbf{u}^\top  \mathbf{L}^\top \, \mathbf{j}_K \right) \mathbf{J}_K \right]
    \nonumber                                                                                                                                                                                                                                                                        \\
                 & = \frac{5}{K^4}\left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{E} \, \mathbf{L}^\top \, \mathbf{j}_K \right) \mathbf{J}_K
    \quad,
\end{align}
%
where we define
%
\begin{align}
    \mathbf{E} \equiv \mathbb{E}\left[ \mathbf{u} \mathbf{u}^\top  \mathbf{A} \,  \mathbf{u} \mathbf{u}^\top \right]
\end{align}
%
and
%
\begin{align}
    \mathbf{A} \equiv \mathbf{L}^\top \, \mathbf{J}_K \, \mathbf{L}
    \quad.
\end{align}
%
The components of this matrix are
%
\begin{align}
    E_{ij} & =
    \sum\limits_{k,l}A_{kl} \mathbb{E}(u_i u_j u_k u_l)
    \nonumber  \\
           & =
    \sum\limits_{k,l}A_{kl}
    \Big(
    g_4 \, \delta_{i, j} \, \delta_{k, l} \, \delta_{i, k}
    +
    g_2^2 \big(
        \delta_{i, j} \, \delta_{k, l} \, \bar{\delta}_{i, k}
        +
        \delta_{i, k} \, \delta_{j, l} \, \bar{\delta}_{i, j}
        +
        \delta_{i, l} \, \delta_{j, k} \, \bar{\delta}_{i, j}
        \big)
    \Big)
    \nonumber  \\
           & =
    2 A_{ij} + \sum\limits_k A_{kk}
    \quad,
\end{align}
%
where we used the fact that $\mathbf{A} = \mathbf{A}^\top$. We may thus write
%
\begin{align}
    \mathbf{E} & =
    2 \, \mathbf{A} + \mathrm{tr}(\mathbf{A}) \mathbf{I}_K
    \nonumber      \\
               & =
    2 \, \mathbf{L}^\top \, \mathbf{J}_K \, \mathbf{L} + \mathrm{sum}(\pmb{\Sigma}) \mathbf{I}_K
    \quad.
\end{align}
%
Inserting this back into the expression for $\mathbf{P}_4$, we obtain
%
\begin{align}
    \mathbf{P}_4 & =
    \frac{5}{K^4}
    \bigg(
    2 \, \big(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{L}^\top \, \mathbf{J}_K \, \mathbf{L} \, \mathbf{L}^\top \, \mathbf{j}_K \big)
    +
    \mathrm{sum}(\pmb{\Sigma}) \big(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{L}^\top \, \mathbf{j}_K \big)
    \bigg) \mathbf{J}_K
    \nonumber        \\
                 & =
    3 \, \bar{\Sigma}^2 \mathbf{J}_K
    \quad.
\end{align}
%
The general term in this sequence is
%
\begin{align}
    \mathbf{P}_n & =
    \mathbf{J}_K
    \begin{cases}
        0                                       & n \, \mathrm{odd}
        \\
        g_n (n + 1) \, \bar{\Sigma}^\frac{n}{2} & n \, \mathrm{even}
        \quad.
    \end{cases}
\end{align}
%


\subsection{The $\mathbf{Q}$ expectation}
%
Let us again compute the first several terms of this expectation matrix.
%
\begin{align}
    \mathbf{Q}_0 & = \mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top \right]
    \nonumber                                                                                                                                                  \\
                 & = \mathbf{L} \, \mathbb{E}\left[  \mathbf{u} \right] \, \mathbf{j}_K^\top
    \nonumber                                                                                                                                                  \\
                 & = \mathbf{0}
    %
    \\[1em]
    %
    \mathbf{Q}_1 & = \frac{1}{K}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top \right]
    \nonumber                                                                                                                                                  \\
                 & = \frac{1}{K}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{L} \, \mathbf{u} \right] \, \mathbf{j}_K^\top
    \nonumber                                                                                                                                                  \\
                 & = \frac{1}{K}\mathbb{E}\left[ \mathbf{L} \, \mathbf{u} (\mathbf{u}^\top \mathbf{L}^\top \mathbf{j}_K) \right] \, \mathbf{j}_K^\top
    \nonumber                                                                                                                                                  \\
                 & = \frac{1}{K}\mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \mathbf{u}^\top \mathbf{L}^\top  \right] \, \mathbf{J}_K
    \nonumber                                                                                                                                                  \\
                 & = \frac{1}{K}\mathbf{L} \, \mathbb{E}\left[  \mathbf{u} \mathbf{u}^\top   \right]  \, \mathbf{L}^\top \mathbf{J}_K
    \nonumber                                                                                                                                                  \\
                 & = \frac{1}{K}\mathbf{L} \, \mathbf{L}^\top \mathbf{J}_K
    \nonumber                                                                                                                                                  \\
                 & = \frac{1}{K}\pmb{\Sigma} \, \mathbf{J}_K
\end{align}
%
where we made liberal use of the fact that $\left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)$ is a scalar. Continuing,
%
\begin{align}
    \mathbf{Q}_2 & = \frac{2}{K^2}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^2 \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top \right]
    \nonumber                                                                                                                                                                                         \\
                 & =  \frac{2}{K^2}\mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \, (\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}) (\mathbf{u}^\top \mathbf{L}^\top \mathbf{j}_K) \mathbf{j}_K^\top \right]
    \nonumber                                                                                                                                                                                         \\
                 & = \frac{2}{K^2}\mathbf{L} \, \mathbb{E}\left[ \mathbf{u} \, \mathbf{j}_K^\top \mathbf{L} \, \mathbf{u} \mathbf{u}^\top \right] \, \mathbf{L}^\top \mathbf{J}_K
    \nonumber                                                                                                                                                                                         \\
                 & = \mathbf{0}
\end{align}
%
since again the components of the expectation in the next-to-last line involve products of three standard normal random variables.
Next,
\begin{align}
    \mathbf{Q}_3 & = \frac{3}{K^3}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^3 \mathbf{L} \, \mathbf{u} \, \mathbf{j}_K^\top \right]
    \nonumber                                                                                                                                                      \\
                 & = \xxx{...}
    \nonumber                                                                                                                                                      \\
                 & =
    \frac{3}{K^3}\mathbf{e} \, \mathbf{j}_K^\top
\end{align}
%
% 3 \, \pmb{\Sigma} \, \mathbf{J}_K \, \pmb{\Sigma} \, \mathbf{J}_K
%
where we define
%
\begin{align}
    \mathbf{e} \equiv \mathbb{E}\left[
        \left(
        \mathbf{v}^\top \mathbf{u} \mathbf{u}^\top \mathbf{v}
        \mathbf{v}^\top \mathbf{u}
        \right)
        \mathbf{L} \mathbf{u}
        \right]
\end{align}
%
and
%
\begin{align}
    \mathbf{v} \equiv \mathbf{L}^\top \, \mathbf{j}_K
    \quad.
\end{align}
%
The components of $\mathbf{e}$ are given by
%
\begin{align}
    e_{i} & =
    \sum\limits_{j,k,l,m}L_{ij} v_k v_l v_m \mathbb{E}(u_j u_k u_l u_m)
    \nonumber \\
          & =
    \sum\limits_{j,k,l,m}L_{ij} v_k v_l v_m
    \Big(
    g_4 \, \delta_{j, k} \, \delta_{l, m} \, \delta_{j, l}
    +
    g_2^2 \big(
        \delta_{j, k} \, \delta_{l, m} \, \bar{\delta}_{j, l}
        +
        \delta_{j, l} \, \delta_{k, m} \, \bar{\delta}_{j, k}
        +
        \delta_{j, m} \, \delta_{k, l} \, \bar{\delta}_{j, k}
        \big)
    \Big)
    \nonumber \\
          & =
    \xxx{...}
\end{align}
%
We may thus write
%
\begin{align}
    \mathbf{e} & = 3 \, \pmb{\Sigma} \, \mathbf{J}_K \, \pmb{\Sigma} \, \mathbf{j}_K
    \quad.
\end{align}
%
Inserting this back into the expression for $\mathbf{Q}_3$, we obtain
%
\begin{align}
    \mathbf{Q}_3 & =
    \frac{9}{K^3} \, (\pmb{\Sigma} \, \mathbf{J}_K)^2
\end{align}
%
Based on this pattern, the general term in $\mathbf{Q}$ is
%
\begin{align}
    \mathbf{Q}_n & =
    \begin{cases}
        \mathbf{0}                                                                   & n \, \mathrm{even}
        \\
        \frac{(n + 1)g_{n+1}}{K^n} \, (\pmb{\Sigma} \, \mathbf{J}_K)^\frac{n + 1}{2} & n \, \mathrm{odd}
        \quad.
    \end{cases}
\end{align}

\subsection{The $\mathbf{R}$ expectation}
%
The first several terms in $\mathbf{R}$ are given by
%
\setlength{\abovedisplayskip}{1em}
\begin{align}
    \mathbf{R}_0 & = \mathbb{E}\left[ \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
    \nonumber                                                                                                                                                                \\
                 & = \mathbf{L} \, \mathbb{E}\left[  \mathbf{u} \, \mathbf{u}^\top \right] \, \mathbf{L}^\top
    \nonumber                                                                                                                                                                \\
                 & = \pmb{\Sigma}
    %
    \\[1em]
    %
    \mathbf{R}_1 & = \frac{1}{K}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right) \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
    \nonumber                                                                                                                                                                \\
                 & = \mathbf{0}
    %
\end{align}
%
since its elements are all products of three standard normal random variables. Next,
%
\begin{align}
    %
    \mathbf{R}_2 & = \mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^2 \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
    \nonumber                                                                                                                                                       \\
                 & = \xxx{...}
    \nonumber                                                                                                                                                       \\
                 & = 2 \pmb{\Sigma} \mathbf{J}_K \pmb{\Sigma} + \mathrm{sum}(\pmb{\Sigma}) \pmb{\Sigma}
\end{align}
%
\xxx{By trial and error, I found that}
%
\begin{align}
    %
    \mathbf{R}_4 & = \frac{5}{K^4}\mathbb{E}\left[ \left(\mathbf{j}_K^\top \mathbf{L} \, \mathbf{u}\right)^4 \mathbf{L} \, \mathbf{u} \, \mathbf{u}^\top \mathbf{L}^\top \right]
    \nonumber                                                                                                                                                                    \\
                 & = \xxx{...}
    \nonumber                                                                                                                                                                    \\
                 & = \frac{5}{K^4} \Big( 12 (\pmb{\Sigma} \mathbf{J}_K)^2 \pmb{\Sigma} + 3 \, \mathrm{sum}(\pmb{\Sigma})^2 \pmb{\Sigma} \Big)
\end{align}
%
Based on this pattern, the general term in $\mathbf{R}$ is
%
\begin{align}
    \mathbf{R}_n & =
    \begin{cases}
        \mathbf{0}                                                                                                                                     & n \, \mathrm{odd}
        \\
        \frac{n (n + 1) g_{n}}{K^n} \, (\pmb{\Sigma} \, \mathbf{J}_K)^\frac{n}{2}\pmb{\Sigma} + (n + 1) g_{n} \, \bar{\Sigma}^\frac{n}{2} \pmb{\Sigma} & n \, \mathrm{even}
        \quad.
    \end{cases}
\end{align}

\subsection{The final result}
Inserting the matrices $\mathbf{P}$, $\mathbf{Q}$, and $\mathbf{R}$ into the expression
for $\tilde{\pmb{\Sigma}}$ and rearranging,
we obtain
%
\begin{align}
    \tilde{\pmb{\Sigma}}
     & =
    \frac{1}{\mu^2} \pmb{\Sigma}
    +
    \frac{1}{\mu^2}
    \sum\limits_{n=1}^\infty
    \frac{(-1)^n(n + 1)}{\mu^{n}}
    \,
    \bigg[
        g_n \, \bar{\Sigma}^\frac{n}{2} \, (\pmb{\Sigma} + \mu^2\mathbf{J}_K)
        \, +
        \nonumber                 \\[0.5em]
     & \phantom{XXXXXXXXXXXXXXX.}
    K g_{n+1} \mu  \,
    \left(
    \left(\pmb{\Sigma} \, \frac{\mathbf{J}_K}{K^2}\right)^\frac{n + 1}{2}
    +
    \left(\frac{\mathbf{J}_K}{K^2} \, \pmb{\Sigma}\right)^\frac{n + 1}{2}
    \right)
    \, +
    \nonumber                     \\[0.5em]
     & \phantom{XXXXXXXXXXXXXXX.}
    n g_n \, \left(\pmb{\Sigma} \, \frac{\mathbf{J}_K}{K^2}\right)^\frac{n}{2}\pmb{\Sigma}
    \bigg]
    \quad.
\end{align}

\bibliography{bib}
\end{document}